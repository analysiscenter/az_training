{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains a demonstration of regression models using tensorflow and dataset\n",
    "The models are implemented as methods of the class MyBatch in linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import seaborn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dataset import Dataset\n",
    "import linear as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=[]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (continious target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "TRAINING_EPOCHS = 500\n",
    "DATA_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.load_linear_data returns np.array of feautures and linearly dependent target with error from normal distribution\n",
    "data = lr.load_linear_data(DATA_SIZE)\n",
    "\n",
    "# create dataset object\n",
    "my_dataset = Dataset(index=np.arange(data[0].shape[0]), batch_class=lr.MyBatch)\n",
    "\n",
    "# create train and test indices\n",
    "my_dataset.cv_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.dataset.pipeline.Pipeline at 0x1b8eab20470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and run train pipeline which loads, preprocesses and trains linear regression model\n",
    "my_dataset.train.pipeline() \\\n",
    "    .load(data) \\\n",
    "    .preprocess_linear_data() \\\n",
    "    .train_linear(sess, cost_history) \\\n",
    "    .run(BATCH_SIZE, shuffle=True, n_epochs=TRAINING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true, y_pred, mse, x_features = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run test pipeline which loads, preprocesses and tests the trained model\n",
    "test_batch = my_dataset.test.p \\\n",
    "    .load(data) \\\n",
    "    .preprocess_linear_data() \\\n",
    "    .test_linear(sess, y_true, y_pred, mse, x_features) \\\n",
    "    .run(len(my_dataset.test.indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance ratio: 0.73\n"
     ]
    }
   ],
   "source": [
    "print ('Variance ratio: %.2f' % (np.var(y_pred, ddof=1) /np.var(y_true, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is distibuted in the interval: 0.12 $ ± 0.25 $\n"
     ]
    }
   ],
   "source": [
    "print ('MSE is distibuted in the interval: {0:.2f} $ ± {1:.2f} $'\\\n",
    "        .format(np.mean(np.abs(np.array(y_pred) - np.array(y_true))), 3*np.std(np.abs(np.array(y_pred) - np.array(y_true)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with respect to data's mean is: 3.81%\n"
     ]
    }
   ],
   "source": [
    " print ('MSE with respect to data\\'s mean is: {0:.2f}%'\\\n",
    "        .format(np.mean(np.abs(np.array(y_pred) - np.array(y_true))/np.array(y_true))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create random data for classification\n",
    "data = lr.load_random_data(blobs=False)\n",
    "\n",
    "# create dataset object\n",
    "my_dataset = Dataset(index=np.arange(data[0].shape[0]), batch_class=lr.MyBatch)\n",
    "\n",
    "# create train and test indices\n",
    "my_dataset.cv_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "cost_history = []\n",
    "acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.dataset.pipeline.Pipeline at 0x1b8eab208d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and run train pipeline which loads, preprocesses and trains logistic regression model\n",
    "my_dataset.train.p \\\n",
    "    .load(data) \\\n",
    "    .preprocess_binary_data() \\\n",
    "    .train_logistic(sess, cost_history, acc_history) \\\n",
    "    .run(BATCH_SIZE, shuffle=True, n_epochs=TRAINING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "# create and run test pipeline which loads, preprocesses and tests the trained model\n",
    "test_batch = my_dataset.test.p \\\n",
    "    .load(data) \\\n",
    "    .preprocess_binary_data() \\\n",
    "    .test_logistic(sess, acc) \\\n",
    "    .run(len(my_dataset.test.indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 95%\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY: %.0f%%\" % (100.0 * acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Regression (discret target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lr.load_poisson_data generates sample from poisson distribution and returns a tuple of weights, features and labels\n",
    "data = lr.load_poisson_data()\n",
    "\n",
    "# create dataset object\n",
    "my_dataset = Dataset(index=np.arange(data[1].shape[0]), batch_class=lr.MyBatch)\n",
    "\n",
    "# create train and test indices\n",
    "my_dataset.cv_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.dataset.pipeline.Pipeline at 0x1b8eadae978>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and run train pipeline which loads, preprocesses and trains poisson regression model\n",
    "my_dataset.train.p \\\n",
    "    .load(data[1:]) \\\n",
    "    .train_poisson(sess, cost_history) \\\n",
    "    .run(BATCH_SIZE, shuffle=True, n_epochs=TRAINING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit, y_true, weights = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = my_dataset.test.p.load(data[1:]).test_poisson(sess, y_true, logit, weights).run(len(my_dataset.test.indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance ratio: 0.95\n"
     ]
    }
   ],
   "source": [
    "lmbd = np.dot(data[1], data[0])\n",
    "print ('Variance ratio: %.2f' % (np.var(logit, ddof=1) / np.var(lmbd, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's error with respect to data's mean is: 10.40%\n"
     ]
    }
   ],
   "source": [
    " print ('The model\\'s error with respect to data\\'s mean is: {0:.2f}%'\\\n",
    "        .format(np.mean(np.abs(np.exp(logit) - y_true))/np.mean(y_true)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
