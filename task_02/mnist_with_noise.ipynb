{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkNet: Network for Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abhishek Chaurasia, Eugenio Culurciello, Jun 2017, https://arxiv.org/abs/1707.03718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkNet architecture is inspired by auto-encoders: each encoder (decoder) performs downsampling (upsampling) the feature maps by a factor of 2. At the same time the number of channels increases (decreases) except the outputs of the first encoder-decoder blocks. The main novelty of LinkNet as a segmenation network is a usage of skip connections between encoders and decoders. This\n",
    "approach enables to save spatial information that contains in input image and helps to train neural networks. Each convolutional layer is followed by batch-normalization and ReLU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/01.PNG' width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/02.PNG' width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LinkNet to segmenation of $128 \\times 128$ images with MNIST $28 \\times 28$ at random place (uniformly sampled) with noise generated on the base of MNIST fragments. Each fragment is randomly cutted from random image from the same batch and is rotated by an angle $ \\sim U(0,360^{\\circ})$. Coordinates of top-left corner are sampled from uniform $U(0, 128-s)$ or normal $N\\left(\\frac{128-s}{2}, \\left(\\frac{128-s}{4}\\right)^2\\right)$ distribution where $s$ is equal to width (height) of rotated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../base_networks/segmentation/\")\n",
    "sys.path.append(\"../base_networks/classification/\")\n",
    "\n",
    "from dataset import Pipeline, DatasetIndex, Dataset, B, C, F, V\n",
    "\n",
    "from fcn import FCNModel\n",
    "from unet import UNetModel\n",
    "from linknet import LinkNetModel                                        # TFModel subclass with LinkNet\n",
    "from nmnist import NoisedMnist                                          # Batch subclass with loading and noise actions\n",
    "from plot_functions import plot_noised_image, plot_examples_highlighted # plot functions to demonstrate result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix constants to generate noised images and train LinkNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64     # image size\n",
    "MNIST_SIZE = 65000   # MNIST database size\n",
    "BATCH_SIZE = 64     # batch size for NN training\n",
    "MAX_ITER = 200       # number of iterations for NN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define noise parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level = 1           # the highest level of noise; [0, 1]\n",
    "n_fragments = 80    # number of noise fragments per image  \n",
    "size = 4            # size of noise fragment; 1, ..., 27\n",
    "distr = 'uniform'    # distribution of fragments of image; 'uniform' or 'normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DatasetIndex and Dataset to use pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = DatasetIndex(np.arange(MNIST_SIZE))          # index for images\n",
    "mnistset = Dataset(ind, batch_class=NoisedMnist)   # Dataset with transform actions in NoisedMnist class\n",
    "mnistset.cv_split([0.9, 0.1])                      # divide it into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð¡reate Pipeline template for image loading and transformation. The first parameter of create_noise is the type of noise: 'mnist_noise' - MNIST-based noise, 'random_noise' - uniform random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_template = (Pipeline()\n",
    "                 .load_images()                    # load MNIST images from file\n",
    "                 .random_location(IMAGE_SIZE)      # put MNIST at random location\n",
    "                 .create_mask()                    # create mask for MNIST image location\n",
    "                 .create_noise('mnist_noise',\n",
    "                            level,\n",
    "                            n_fragments, \n",
    "                            size, \n",
    "                            distr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot example of noised images (train images are greyscale but we highlight true digit in yellow to plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "noise = []\n",
    "(load_template << mnistset.train).get_images(images).get_noise(noise).next_batch(10, shuffle=True)\n",
    "plot_noised_image(images[0][0], noise[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create config for placeholders of the model. Key in dict is name of the created placeholder.\n",
    "* '<b>shape</b>' - shape of the input of model\n",
    "* '<b>type</b>' - tf.dtype of input\n",
    "* '<b>data_format</b>' - one of channels_last (default) or channels_first\n",
    "* '<b>name</b>' - name of the placeholder after reshaping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders_config = {\n",
    "                       'images': {'shape': (IMAGE_SIZE, IMAGE_SIZE, 1),\n",
    "                                 'type': 'float32',\n",
    "                                 'data_format': 'channels_last',\n",
    "                                 'name': 'reshaped_images'},\n",
    "                \n",
    "                       'masks': {'shape': (IMAGE_SIZE, IMAGE_SIZE, 2),\n",
    "                                 'type': 'int32',\n",
    "                                 'data_format': 'channels_last',\n",
    "                                 'name': 'targets'}\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model_config.\n",
    "* '<b>placeholders</b>' - dict of placeholders configs\n",
    "* '<b>n_classes</b>' - number of output classes\n",
    "* '<b>b_norm</b>' - enable batch normalization\n",
    "* '<b>loss</b>' - loss function\n",
    "* '<b>optimizer</b>' - loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_config = {'inputs': placeholders_config,\n",
    "                'batch_norm': True,\n",
    "                'loss': 'softmax_cross_entropy',\n",
    "                'optimizer': 'Adam'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feed dicts. The key is name of the tensor in tf graph, value is batch component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feed_dict = {'images': B('images'),\n",
    "                   'masks': B('masks')}        \n",
    "\n",
    "test_feed_dict = {'images': B('images'),\n",
    "                  'masks': B('masks')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_train = ((load_template << mnistset.train)\n",
    "            .add_noise()\n",
    "            .init_variable('train_loss_history', init_on_each_run=list)\n",
    "            .init_model('static',                                          # model mode\n",
    "                         LinkNetModel,                                      # TFModel subclass with LinkNet\n",
    "                        'NN',                                         # model name\n",
    "                        config=model_config)\n",
    "            .train_model('NN',                                        # model name\n",
    "                         fetches='loss',                                   # tensors to get value \n",
    "                         feed_dict=train_feed_dict,                        \n",
    "                         save_to=V('train_loss_history'), mode='a'))                  # name of pipeline variable to save loss value\n",
    "\n",
    "ppl_test = ((load_template << mnistset.test)\n",
    "            .add_noise()\n",
    "            .import_model('NN', ppl_train)\n",
    "            .init_variable('test_loss_history', init_on_each_run=list)\n",
    "            .predict_model('NN', \n",
    "                           fetches='loss',\n",
    "                           feed_dict=test_feed_dict,\n",
    "                           save_to=V('test_loss_history'), mode='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LinkNet on noised data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training of the model\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    ppl_train.next_batch(BATCH_SIZE, n_epochs=None, shuffle=True)                              # training step\n",
    "    ppl_test.next_batch(100, n_epochs=None, shuffle=True)                                      # compute test loss\n",
    "    \n",
    "    train_loss = ppl_train.get_variable('train_loss_history')[-1]                # get current iteration train loss\n",
    "    test_loss = ppl_test.get_variable('test_loss_history')[-1]                   # get current iteration test loss\n",
    "    \n",
    "    if (i+1) % 1 == 0:\n",
    "        print(\"Iter {:3d}: train {:05.3f} test {:05.3f}\".format(i+1, train_loss, test_loss))\n",
    "        \n",
    "stop = time()\n",
    "\n",
    "print(\"Train time: {:05.3f} min\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train and test loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ppl_train.get_variable('train_loss_history'), label='Train loss')\n",
    "plt.plot(ppl_test.get_variable('test_loss_history'), label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline to get some images from test dataset and corresponding masks, noise and mask predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "masks = []\n",
    "noise = []\n",
    "\n",
    "ppl_plot = ((load_template << mnistset.test)                       # load data from file\n",
    "             .get_images(images)                                   # images without noise\n",
    "             .get_masks(masks)                                     # get masks\n",
    "             .get_noise(noise)                                     # get noise\n",
    "             .add_noise()                                          # add noise to images\n",
    "             .import_model('NN', ppl_train)\n",
    "             .init_variable('predictions', init_on_each_run=list)\n",
    "             .predict_model('NN',                                      \n",
    "                           fetches='predicted_prob',\n",
    "                           feed_dict=test_feed_dict,\n",
    "                           save_to=V('predictions'),\n",
    "                           mode='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for 10 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_plot.next_batch(10, n_epochs=None)\n",
    "predictions = ppl_plot.get_variable('predictions')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images with highlighted digit, mask for $28 \\times 28$ image, binary mask prediction and probability prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples_highlighted(images, noise, masks, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
