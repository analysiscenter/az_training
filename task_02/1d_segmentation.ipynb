{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dataset import Pipeline, DatasetIndex, Dataset, B, C, F, V\n",
    "\n",
    "from linknet import LinkNetModel                                  # TFModel subclass with LinkNet\n",
    "from nmnist import NoisedMnist                                          # Batch subclass with loading and noise actions\n",
    "from plot_functions import plot_noised_image, plot_examples_highlighted # plot functions to demonstrate result \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256   # batch size for NN training\n",
    "MAX_ITER = 100     # number of iterations for NN training\n",
    "SIZE = 50000\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 50000/50000 [00:01<00:00, 49847.59it/s]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "masks = []\n",
    "for i in tqdm(range(SIZE)):\n",
    "    data = np.sin(np.random.uniform(0, 2*np.pi) + np.arange(0, IMAGE_SIZE) / IMAGE_SIZE * 2*np.pi )\n",
    "    mask = data > 0.25\n",
    "    images.append(data)\n",
    "    masks.append(mask)\n",
    "images = np.array(images)\n",
    "masks = np.array(masks)\n",
    "masks = np.stack([1-masks, masks], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = DatasetIndex(np.arange(SIZE))          # index for images\n",
    "dset = Dataset(ind, batch_class=NoisedMnist)   # Dataset with transform actions in NoisedMnist class\n",
    "dset.cv_split([0.9, 0.1])                      # divide it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_template = Pipeline().load(src=(images, masks), components=('images', 'masks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feed_dict = {'input_image': B('images'),\n",
    "                   'targets': B('masks')\n",
    "                  }        \n",
    "\n",
    "test_feed_dict = {'input_image': B('images'),\n",
    "                  'targets': B('masks')\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_train = ((load_template << dset.train)\n",
    "            .init_variable('train_loss_history', init_on_each_run=list)\n",
    "            .init_model('static',                                          # model mode\n",
    "                        LinkNetModel,                                      # TFModel subclass with LinkNet\n",
    "                        'linknet',                                         # model name\n",
    "                        config={'loss': 'softmax_cross_entropy',\n",
    "                                'optimizer': 'Adam',\n",
    "                                'n_classes': 2,\n",
    "                                'dim': 1,\n",
    "                                'b_norm': False, \n",
    "                                'images_shape': (IMAGE_SIZE, )})\n",
    "            .train_model('linknet',                                        # model name\n",
    "                         fetches='loss',                                   # tensors to get value \n",
    "                         feed_dict=train_feed_dict,                        \n",
    "                         append_to=V('train_loss_history')))                  # name of pipeline variable to save loss value\n",
    "\n",
    "ppl_test = ((load_template << dset.test)\n",
    "            .import_model('linknet', ppl_train)\n",
    "            .init_variable('test_loss_history', init_on_each_run=list)\n",
    "            .predict_model('linknet', \n",
    "                           fetches='loss',\n",
    "                           feed_dict=test_feed_dict,\n",
    "                           append_to=V('test_loss_history')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  10: train 1.120 test 1.027\n",
      "Iter  20: train 0.693 test 0.666\n",
      "Iter  30: train 0.277 test 0.206\n",
      "Iter  40: train 0.075 test 0.059\n",
      "Iter  50: train 0.041 test 0.044\n",
      "Iter  60: train 0.037 test 0.037\n",
      "Iter  70: train 0.032 test 0.025\n",
      "Iter  80: train 0.020 test 0.028\n"
     ]
    }
   ],
   "source": [
    "# training of the model\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    ppl_train.next_batch(BATCH_SIZE, n_epochs=None)                              # training step\n",
    "    ppl_test.next_batch(100, n_epochs=None)                                      # compute test loss\n",
    "    \n",
    "    train_loss = ppl_train.get_variable('train_loss_history')[-1]                # get current iteration train loss\n",
    "    test_loss = ppl_test.get_variable('test_loss_history')[-1]                   # get current iteration test loss\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"Iter {:3d}: train {:05.3f} test {:05.3f}\".format(i+1, train_loss, test_loss))\n",
    "        \n",
    "stop = time()\n",
    "\n",
    "print(\"Train time: {:05.3f} min\".format((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ppl_train.get_variable('train_loss_history'), label='Train loss')\n",
    "plt.plot(ppl_test.get_variable('test_loss_history'), label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "masks = []\n",
    "noise = []\n",
    "\n",
    "ppl_plot = ((load_template << dset.test)\n",
    "             .import_model('linknet', ppl_train)\n",
    "             .init_variable('predictions', init_on_each_run=list)\n",
    "             .predict_model('linknet',                                      \n",
    "                           fetches='prob_predictions',\n",
    "                           feed_dict=test_feed_dict,\n",
    "                           append_to=V('predictions')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppl_plot.next_batch(10, n_epochs=None)\n",
    "predictions = ppl_plot.get_variable('predictions')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_examples_highlighted(images, noise, masks, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
