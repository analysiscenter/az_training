Всем привет! В этом видео мы рассмотрим архитектуру VGG.
Для начала мы кратко поговорим об истории её появления, затем более подробно о самой архитектуре и в конце остановимся на её достоинствах и недостатках.

VGG была разработана студентами из Visual Geometry Group и интересна тем, что в 2014 году заняла призовые места в imagenet localization and classification tasks.

Посмотрев на все архитектуры до VGG можно заметить, что они строились по одному и тому же принципу. Первое на что можно обратить внимание это то, что в начале сети всегда используются большие свертки, размер которых уменьшается от входа сети к выходу. Второе это то, что общая глубина сети была сравнительно не большой, в среднем от 5 до 8 слоев.

В отличии от них VGG это глубокая сеть, порядка 11 - 19 слоев, состоящая из небольших конволюций размера 3х3.

Использование больших сверток в ранних архитектурах объясняется желанием получить большой receptive field, это регион на входном изображении который участвует в получении текущего пикселя на карте признаков. И чем он больше, тем лучше сеть учитывает корреляции пикселей.
Увеличить rf можно используя большие свертки, но это требует больших вычислительных мощностей.

Авторы VGG доказали, что можно получить бОльший rf применяя много последовательных сверток малого размера. Таким образом две подряд идущие свертки 3х3 заменяют одну 5х5, и при этом они требуют примерно на 28% меньше памяти и, соответственно, требуют меньше вычислений.
В своей статье авторы приводят 6 различных вариаций VGG от 11 до 19 слоев, включая 4 полносвязных слоя в конце. В среднем во всех её реализациях около 140 м параметров. Из них всего 24% весов содержится в сверточных слоях, а все остальные параметры находятся в полносвязных.

Подводя итоги давайте посмотрим на плюсы и минусы этой архитектуры:
Сначала поговорим о достоинствах:
*Во-первых, использование маленьких сверток позволяют увеличить rf, не сильно увеличивая кол-во параметров.
*Во-вторых, использование двух сверточных слоев, вместо одного позволяет делать два нелинейных преобразования, таким образом увеличивая пространство поиска.
*И в третьих. Архитектура очень проста в реализации.
Теперь о недостатках:
* Если сделать сеть глубже, то появится проблема затухания градиента.? О которой мы поговорим в следущем видео?
* Еще одним существенным недостатком является большое количество параметров в полносвязных слоях. Отсюда вытекает новая проблема - сеть медленно обучается. Даже сами авторы обучали эту сеть около двух недель на 4-х видеокартах nvidia titan.

Благодоря простоте реализации сейчас существует множество уже обученных вариантов VGG. Таким образом она остается актуальной до сих пор и используется в новых архитектурах для начальной обработки изображений, например в таких сетях как SSD или FCN-8.


И чем он большое, тем лучше сеть учитывает корреляции пикселей.
The more - the merrier, meaning that | network takes into consideration this pixel correlation.

Увеличить rf можно используя большие свертки, но это требует больших вычислительных мощностей.
Increasing RF can be obviously reached by enlarging convolutions |, but this consumes a lot of computational capabilities.

Авторы VGG доказали, что можно получить бОльший rf применяя много последовательных сверток малого размера. Таким образом две подряд идущие свертки 3х3 заменяют одну 5х5, и при этом они требуют примерно на 28% меньше памяти и, соответственно, требуют меньше вычислений.
VGG creators proved community wrong, by achieving lArger RF with applying multiple sequential convolutions of smaller size. This way 2 sequential 3x3 convolutions substitute 1 5x5 and by doing so, they as well save approximately 28% less of memory and, respectively - less computations.

В своей статье авторы приводят 6 различных вариаций VGG от 11 до 19 слоев, включая 4 полносвязных слоя в конце. В среднем во всех её реализациях около 140 м параметров. Из них только 24% весов содержатся в сверточных слоях, а остальные параметры находятся в полносвязных слоях.
In their outcome, authors showcase 6 different VGG variations of 11 to 19 layers, including 4 fully-connected layers in the end. On average, each one of its realisations contains around 140 mil parameters. Among all of them only 24% of weights are contained in the convolutional layers, while the rest are maintained in fully-convolutional.

Подводя итоги давайте посмотрим на плюсы и минусы этой архитектуры: Сначала поговорим о достоинствах:
We’ve went thought major features and to make it even we should summarise our review result. Let’s have a look at bonuses and limitations. Sure thing - bonuses first:

*Во-первых, использование маленьких сверток позволяют увеличить rf, не сильно увеличивая кол-во параметров.
Feature 1 - usage of smaller convolutions allows to increase RF, meanwhile keep number of parameters even

*Во-вторых, использование двух сверточных слоев, вместо одного позволяет делать два нелинейных преобразования, таким образом увеличивая пространство поиска.
Feature 2 - enjoyment of 2 convolutional layers, instead of 1 - allows execution of 2, instead of 1, non-linear transformations, thus enhancing the ________(пространство поиска)

*И в третьих. Архитектура очень проста в реализации.
Feature 3 - most important, the architecture is UBER-easy to replicate.

Теперь о недостатках:
Now, check-out the limits

* Если сделать сеть глубже, то появится проблема затухания градиента.? О которой мы поговорим в следующем видео?
If you decide to increase the depth of the net - you will, most definitely meet the vanishing gradients проблема затухания градиента, which is the subject of our next video

* Еще одним существенным недостатком является большое количество параметров в полносвязных слоях. Отсюда вытекает новая проблема - сеть медленно обучается. Даже сами авторы обучали эту сеть около двух недель на 4-х nvidia titan.

Благодаря простоте реализации сейчас существует множество уже обученных вариантов VGG.

Таким образом она остается актуальной до сих пор и используется в новых архитектурах для начальной обработки изображений, например в задачах сегментации (SSD или FCN-8).