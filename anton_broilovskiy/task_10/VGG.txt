VGG.
(Первый слайд) - Всем привет! В этом видео мы рассмотрим архитектуру VGG.
    Для начала мы кратко поговорим об истории её появления, затем более подробно о самой архитектуре и в конце остановимся на её достоинствах и недостатках.

    VGG была разработана студентами из Visual Geometry Group и интересна тем, что в 2014 году заняла 1 место на соревновании imagenet в задаче localization task and second in classification task.

(Второй слайд) - AlexNet и его последователи с

(Третий слайд) - Архитектура VGG же сильно отличается от них, это глубокая сеть состоящая из конволюций размера 3х3, пуллинга для уменьшения изображения и полносвязных слоев в конце.

(Четвертый слайд) - !сделать его! Использование больших сверток в ранних архитектурах объясняется желанием получить большой receptive field,( это регион на входном изображении который участвует в получении текущего пикселя на карте признаков) this is a region in the input image that a particular networks feature is looking at. И чем он большое, тем лучше сеть учитывает корреляции пикселей. 
Увеличить rf можно используя большие свертки, !! но это требует больших вычислительных мощностей.

(Пятый слайд) - Авторы VGG доказали, что можно получить больший rf применяя много последовательных сверток малого размера. Таким образом две подряд идущие свертки 3х3 заменяют одну 5х5, при этом они требуют примерно на 28% меньше памяти и, соответственно, требуют меньше вычислений.

(Шестой слайд) - Как уже было упомянуто ранее, сеть состоит из подряд идущих сверточных слоев 3х3
    В своей статье авторы приводят 6 различных вариаций VGG от 11 до 19 слоев. В среднем во всех её реализациях около 140 м параметров. Из них около 24% от всей сети занимают веса сверточны слоев, а остальные параметры находятся в полносвязных слоях.

(Седьмой слайд) не нравится переход
Подводя итоги давайте посмотрим на плюсы и минусы этой архитектуры:
VGG есть несколько достоитсв:
*Первое и самое главное это то, что использование маленьких сверток позволяют увеличить rf, не сильно увеличивая кол-во параметров.
*Второе. Использование двух сверточных слоев, вместо одного позволяет делать два нелинейных преобразования, вместо одного, это увеличивает пространство поиска.
*И третье. Архитектура очень проста в реализации.
Теперь о недостатках:
* Например если сделать сеть глубже, то появится проблема затухания градиента.? О которой мы поговорим в следущем видео?
* Еще один существенный недостаток это большое количество параметров в полносвязных слоях. Отсюда вытекает еще одна проблема - сеть медленно обучается. Даже сами авторы обучали эту сеть около двух недель.

(Восьмой слайд) - Актуальность сейчас
   Но несмотря на недостатки, благодоря простоте реализации сейчас существует множество уже обученных реализаций VGG. Таким образом она остается актуальной до сих пор и используется в новых архитектурах для начальной обработки изображений, например в задачах сегментации (SSD или FCN-8).

VGG.
1.
Hellow everyone! We are glad to see you in our video.

And today we consider one of the main and standard architecture in famaly convolution neural networks, with which still compare new models. It is VGG, what received his name in honor to Visual Geometry Group who participated with it on ImageNet in twenty fuorteen and took first place in localization task, and second in classifiaction task.

2.
Before VGG is appeared, all models, such as LeNet or AlexNet used the same strategy for building achitecture. It was big convolutions in first layers and small network depth in average from 5 to 8 layers. It was because big convolutions have a many parameters and this not allow build models deeper.
3.
The authors of this architecture look at the problem and solve it by using stack of 3x3 convolution. This decision allowed increase the network depth.

Apply of big convolution layers can be explained by fact that pixels on input image do not carry information separately, but they are strongly correlated with neighboring pixels. And than bigger the kernel of convolution, than better it analyze correlations between pixels. Using less kernel of convolution may cause loss some useful information.
But than you can ask: "Why than 3x3 convolutions be better?"

For answer on this question, first of all we introduce the definition:
Receptive Field ??is a region in the input image that a particular CNN's feature is looking at??

Большой rf это хорошо. Но для этого нужно больше вычислительной мощности.

4.
And now we analyze how 3x3 convolutions can change biggest conv.
It turns out, that stack of two 3x3 conv. layers using consistently has an effective receptive field of 5x5. Three such layers have a 7x7 effective receptive field. Thereforce we can use two convolution of 3x3 instead one 5x5 and have a same receptive field. But in this approach have a two advantage:
First, we incorporate two or more non-linear rectification layers instead of a single one.
Second, we decrease the number of parameters. For example if we took two 3x3 kernels with one filter it have 18 parameters. In the same time one 5x5 kernel have 25 parameters. This method allow decrease about 81% of weights.

This two properties allow build deeper and more complex networks.

5.
But this architecture is not perfect, it have one big defect. If we make model deeper, it will train worse, than same but less model. It problem named 'vanishing gradient' and we talk about it in the next video.

7.
And in conclusion we will sum up the results:
    * Small kernels allow using more non-linearites
    * VGG is frist of deep architecture, it have 19 layers in the largest implementation
    * VGG have about 140M parameters.
    * But if we do model deeper appear vanishing gradient problem

VGG.
В этом видео мы рассмотрим архитектуру VGG.
Для начала кратко мы посмотрим на историю её появления, затем поговорим о самой архитектуре и в конце остановимся на достоинствах и недостатках этой архитектуры

Предшественниками данной архитектуры были такие архитектуры как LexNet, AlexNet и ZF Net, которые имели не глубокую архитектуру, но насчитывали очень много параметров из-за больших (ядер?) сверточных слоев в начале сети и это заметно замедляло обучение. Исследователи из оксфорда решили попробовать совершенно другой подход, учитывая недостатки существующих сетей.

В результате они придумали новое архитетурное решение.
Это была нейронная сеть в которой:
    * Используются ТОЛЬКО свертки небольшого размера, а именно 3х3.
    * Для уменьшения размера карт используется слой пуллинга размером 2х2, который разделяет несколько подряд идущих сверточных слоев.
    * Глубина варьируется от 11 до 19 слоев. """Может быть сейчас это звучит не так убедительно, но исследователи опирались на LexNet or AlexNet в которых было всего по 6-8 слоев."""
    * Количество параметров 138M.

Большой размер ядер начальных слоев в более ранних архитектурах агрументировался тем, что пиксели на исходном изображении не несут в себе много информации, но они сильно скоррелированны с соседними пикселями, поэтому мы можем взять большое окно свертки для анализа этой корреляции и вместе с этим избавится от шумных и неинформативных пикселей. И если брать маленькое окно, мы рискуем пропустить какую-то информацию. Тогда появляется вопрос: "Почему использование сверточных слоев с маленьким размером ядра работает?".

Для ответа на этот вопрос давайте подробнее разберемся в том, как реализованна архитектура VGG, но для начала введем определение:
Receptive field - это набор пикселей на раельном изображении, которые участвовали в расчете определенного элемента в фильтре. (То есть если мы готовим о первом фильтре, то rf самого первого пикселя будет крайний левый угол изображения, а именно те пиксели, к которым самый первый раз применялась операция конволюции).

Посмотрев на архитектуру, можно заметить, что она состоит из блоков разного размера, каждый из которых представлеят из себя несколько подряд идущих сверточных слоев и заканчивается maxpooling-om.
It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5. Thee such layers have a 7x7 effective receptive field. So what we gained by using stack of 3x3 conv intead single 7x7? First, we incorporate thee non-linear rectification layers instead of a single one. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the tack is parametrised by 3(3^2*C^2) = 27 C^2 weights; at the same time, a single 7 × 7 conv. layer would require 7^2*C^2 = 49 C^2 parameters, i.e. 81% more.

Благодоря использованию маленьких сверток идущих одна за другой, мы увеличиваем глубину сети, что позволяет нам выучивать более высокоуровневые признаки и давать более точный ответ.

Но у данной сети есть ряд недостатков:
    * Если сделать VGG очень глубокой, она покажет качество гораздо хуже, чем её аналог с 20 слоями за разумное время. Это происходит из-за того, что градиент, который расчитывается на нижних слоях, доходит до верхних слоев с очень маленькими значениями, которых не хватает для того, чтобы изменить веса в нужном направлении. Такая проблебма назвается "затухание градиента". Эта проблема решается в последущих сетях, о которых мы будем говорить в дальшейнем.
    * Все таки подряд идущие конволюции 3х3 не полностью заменяют конволюции большего размера. Они хуже используют пространственные зависимости. Частично этот недостаток решается за счет большего количества нелинейностей.

В заключение давайте подведем итоги:
1. VGG содержит не большое количество параметров за счет использования только небольших конволюций.
2. Наличие только небольших конволюций повзоляет увеличивать глубину сети, тем самым повышая качество.
3. Сильное увеличение глубины сети может привести к затуханию градиента.

"""
Для продолжения есть несколько вариантов:
* Можно написать про конволюции 1х1
* Про пуллинг, так как я так и не рассказал об устройстве сети
* или еще что-нибудь
"""