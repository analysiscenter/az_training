Inception.
Сегодня я расскажу вам о другой архитектуре, разработанной командой из гугл в один год с VGG, о которой мы говорили ранее, и назвается она Inception-v1 или GoogLeNet.
В ближайшие пять минут мы рассмотрим особенности архитектуры, её положительные и отрицательные стороны.

Если до inception'а исследователи старались делать архитетуру сетей не очень сложной, для этого достаточно посмотреть на AlexNet или VGG(хз вставлять ли), в которых сверточные слои чередуются с полносвязными, в Inception'e же все координально меняется. Его структура состоит из повторяющихся блоков, в которых происходят парралельные операции. Такого подхода не было еще ни в одной архитектуре.

""Basically, at each layer of a traditional ConvNet, you have to make a choice of whether to have a pooling operation or a conv operation (there is also the choice of filter size). What an Inception module allows you to do is perform all of these operations in parallel.""

Inception-block состоит из 3-х параллельных конволюций с размерами ядра 1х1, 3x3 и 5х5 соответственно, а так же pooling слоя. Затем слои конкатятся друг с другом и получившиеся карты отправляются на вход следущего блока.
Наличие конволюций разного размера можно объяснить тем, что один блок пытается одновременно выявить признаки разного размера, чтобы затем соединить их вместе. Конволюции с разными размерами ядра будут реагировать на признаки разного размера, так, конволюция с ядром 1х1 будет изменять канальную зависимость признаков, преобразовая значения карт. Свертка с ядром 3х3 будет реагировать на пространственные зависимости небольшого размера, так как будет имень небольшой receptive field, а свертка с ядром 5х5 будет выявлять пространственные корреляции большего размера, благородя большему размеру receotive field'a. Слой maxpooling'a объясняется ""since pooling operations have been essential for the success in current state of the art convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too""

Это, так называемая, “naïve” idea.
На первый взгляд такой блок идеален и должен хорошо работать, но это не так. Потому что после прохождения всех конволюционных слоев на выходе будет очень большое количество параметров, причиной этого является наличие конволюций 5х5 и 3х3, поэтому, для уменьшения количества параметров перед этими конволюциями добовляют конволюцию 1х1, которая помогает решить эту проблему, так как применение такой конволюции не изменит размера карты, мы можем изменить количество самих карт на выходе, тем самым сэкономив на параметрах . Благодоря этому в inception количество параметров меньше в 12 раз, чем в AlexNet-e.
Теперь поговорим подробнее об устройсте сети.

Как мы уже знаем inception построен из блоков, но перед тем как подавать фильтры в inception-blocks исходное изображение уменьшают по средствам конволюционного слоя с ядром 7х7 и страйдом 2 и max-pooling'a с таким же страйдом. Внутри самих блоков и операции конволюции, и пулинга не изменяют размер фильтров. За его изменение отвечают слои max-pooling'a соединяющие блоки.
Вссего the network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100.

Отдельное внимание стоит обратить на слои перед полносвязными, которые называются global average pooling, this enables adapting and fine-tuning our networks for other label sets easily and it was found that a move from fully connected layers to average pooling improved the top-1 accuracy. Идея avg pooling'a пришла из другой сети, которая назыавется Network in network(там есть и другая отсылка на эту статью, которую я не понял).

Так же не трудно заметить, что у сети имеется несколько выходов, которые позволяют обучать сеть по-частям. Как было сказано выше, в inception очень много слоев, а значит градиенты, которые будут доходить до начальных от конечных, будут очень малы. Для того, чтобы избавиться от этой проблемы, авторы решили обучать её по-частям, чтобы градиент не успевал затухать.( вот эту инфу надо проверить!)

Подводя итоги давайте остановимся на приемуществах и недостатках сети:
К приемуществам можно отнести:
	* Несмотря на то, что она глубокая, в ней в 12 раз меньше параметров, чем в AlexNet!
	* Отсутствие полносвязных слоев походу сети, позволяет соркатить количество параметров.
	* Конволюции разных размеров позволяют отлавливать большее количество полезных признаков.
Теперь поговорим о недостатках:
	* Затухание градиента?
	* Агрессивное уменьшение параметров в блоках?
В заключение хочу сказать, что GoogLeNet was one of the first models that introduced the idea that CNN layers didn’t always have to be stacked up sequentially. Coming up with the Inception module, the authors showed that a creative structuring of layers can lead to improved performance and computationally efficiency.(стырил с сайта, возможно надо переписать)

	""На данный момент существует множество имплементаций блоков. Давайте остановимся на каждом подробнее.

	Inception-v1.
	При создании своей архитектуры авторы опирались на идею взятую из статьи Network in Network, where

	Вообще название "Inception" пришло из одноименного фильма и девиз исследователей при построении этой сети был "we need to go deeper",
	""
