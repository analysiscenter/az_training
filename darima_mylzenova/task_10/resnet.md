ResNet (classification):
	-skipconnection
	-bottleneck	
	-WideResnet
	-SEblock
	-stochastic depth


Сегодня я расскажу об архитектуре Резнет.
Сначала немного поговорим о предпосылках ее появления, затем я опишу архитектуру и в конце поговорим о ее недостатках
и способах их исправления.

Первые конволюционные сети имели относительно немного слоев. 
Но после появления таких сетей как VGG, которые показывали улучшение качества с 13, а потом и с 19 слоями, 
исследователи задались вопросом: 

"Is learning better networks as easy as stacking more layers?"
Правда ли , что можно улучшить качество сети, просто добавив в нее еще несколько слоев? 

Однако эксперименты показали, что это не так. Увеличение слоев в VGG демонстрировало худший результат как на тесте, 
так и на трейне. Если бы результат был хуже только на тесте, можно было бы говорить о переобучении.
Но как объяснить бОльшую ошибку на обучении?
Ведь интуиция подсказывает, что более глубокие сети должны обучаться не хуже менее глубоких, т.к.
второе множество вложено в первое. 
То есть имея сеть глубины N, можно сделать из нее сеть глубины N+k такого же качества,
добавив k Identity layers, пропускающих сигнал без изменений. 
Но на практике обучить более глубокие сети аналогичного качества не получается, по крайней мере, за сравнимое время.

Это наблюдение привело авторов ResNet к идее Shortcut(skip) connection: 

Основной building block сети, называемый residual block, состоит из 2 сверточных слоев с нелинейностью между ними и  
shortcut connection которое берет тензор, пришедший на вход блока и складывает его с выходом последнего слоя блока.

Поэтому, например, если в весах некоторого уровня везде будет 0, он просто пропустит дальше неизмененный сигнал.

Благодаря такому shortcut connection сеть обучается быстрее. Происходит это потому что в отличие от обычной сети, когда при обратном проходе градиент уменьшается от слоя к слою, градиент через shortcut connection проходит без изменений. Тогда на последний слой residual block-а приходит сумма градиентов - одно слагаемое со следующего слоя и одно, проброшенное через skipconnection, от более поздних слоев. 

Строя сеть из таких блоков, авторы добиваются того, что 34-слойная сеть показывает лучшее качество, чем 18-слойная как на трейне, так и на тесте. 

Авторы продолжают эксперименты, увеличивая глубину сети. При этом они облегчают сами блоки, заменяя 2 3х3 конволюции одной конволюцией 3х3 и двумя конволюциями 1х1. // 
Bottleneck picture.
И получают, что 101 and 152 -слойный Resnet превосходит по точности 32-слойный ResNet на ImageNet .

Таким образом, благодаря идее shortcut connection, резнет оказался архитектурой, позволяющей строить очень глубокие сети, которые способны эффективно обучаться, имея даже тысячи слоев. Однако чем больше глубина, тем больше времени требуется на обучение сети. Так, например, авторы  в Майкрософт обучали 152-слойный Резнет на ImageNet 1.5 недели.

Позже выяснилось, что определяющей причиной успеха является не глубина резнета, а подходящее число параметров.
Это показала архитектура WideResNet - этот тот же ResNet, но имеющий большее число каналов в блоках конволюции, но гораздо меньшую глубину (напр., 50 вместо 152). Она достигает аналогичного результата за меньшее время за счет эффективной реализации параллельного умножения тензоров на GPU.