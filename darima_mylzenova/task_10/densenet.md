aDenseNet (classification):
	- every layer in the dense block is connected to the next layers (via concat)


Сегодня мы поговорим об архитектуре DenseNet, авторы которой получили Best Paper on CVPR в 2017 году.

Успех Резнета и ему подобных архитектур показал, что сети могут быть глубже, точнее и гораздо быстрее обучаемы благодаря дополнительным связям между ранними и более поздними слоями.	

В DenseNet этот подход трансформируется в простую концепцию: давайте соединять прямыми связями все слои между собой, так мы максимизируем поток информации, приходящий на каждый слой.

Как это реализовано на практике:
// картинка 1
Чтобы сохранить (feed forward) последовательную передачу информации, каждый слой будет получать на вход, помимо выхода предыдущего слоя - входы всех предыдущих слоев и передавать cвой результат всем последующим слоям. 

Архитектура полной сети выглядит так:
// картинка 2
В отличие от резнета, где результат шорткат коннекшна суммировался с выходом слоя, в DenseNet конкатенируют feature maps с ранних слоев.
Для этого необходимо поддерживать одинаковую пространственную размерность feature maps на каждом слое. 
А так как уменьшение размера изображения - важная составляющая конволюционных сетей, в денснет полная связность поддерживается только между слоями отдельных блоков. 
Эти блоки называются Денсблоки, в нем может быть от 6 до 24 слоев Каждый такой слой состоит из BN-relu-конволюции 1x1-bn-relu-конв 3х3.
Конв 1х1 реализует боттлнек, уменающий число параметров посредством снижения числа feature maps, приходящих на вход конв 3х3.
Для стандартизации и предотвращения чрезмерного роста в ширину, фиксируется параметр k, равный числу выходных каналов каждого блока. Тогда л-ый слой получает на вход k(l-1) + k_0 feature maps, где k_0 - число каналов исходного изображения.

Между денсблоками - переходные блоки , состоящие из батчнорма, 1х1 конволюции и макспулинга со страйдом 2, понижающего размерность изображения.

После последнего денсблока применяется global average pooling (это пулинг размером во все изображение) и dense 1000d с softmax. 


Может показаться немного контринтуитивным, такие сети требуют значительно меньшее число параметров, чем другие конволюционные сети. 
Сравнение DenseSnet с state-of-the-art ResNEt на ImageNet показало, что денс нет показывает аналогичную точность на тесте, имея в три раза меньше параметров, чем резнет.
//картинка с графиком 3х

Происходит это потому 













DenseNet

1. Классификация, сегментация
2. Уменьшило число параметров за счет feature reuse
3. Может иметь разное количество денсблоков и соответственно разную глубину:
	- Все слои получают конкатенацию входов от всех предшествующих с той же размерностью feature map. 
	- Feature reuse
4. Затратно по памяти. Есть эффективная по памяти реализация
5. Вывод: качество резнета, используя в 3 раза меньше парметров за счет feaute reuse

MobileNet

















