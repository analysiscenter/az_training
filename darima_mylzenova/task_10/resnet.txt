ResNet (classification):
	-skipconnection
	-bottleneck	
	-WideResnet
	-SEblock
	-stochastic depth


Сегодня я расскажу об архитектуре Резнет.
Сначала немного поговорим о предпосылках ее появления, затем я опишу архитектуру и в конце поговорим о ее недостатках
и способах их исправления.

После появления таких сетей как VGG, содержащих значительно больше слоев, чем предшествующий AlexNet, 
исследователи задались вопросом: 
"Is learning better networks is as esay as stacking more layers?"

Однако эксперименты показали, что это не так. Увеличение слоев в VGG демонстрировало худший результат как на тесте, 
так и на трейне. Если бы результат был хуже только на тесте, можно было бы говорить о переобучении.
Но как объяснить большую ошибку на обучении?
Ведь интуиция подсказывает, что более глубокие сети должны обучаться не хуже менее глубоких, т.к.
второе множество вложено в первое. Имея сеть глубины N, можно сделать из нее сеть глубины N+k такого же качества,
добавив k Identity layers, пропускающих сигнал без изменений. 
Но на практике обучить более глубокие сети аналогичного качества не получается, по крайней мере, за сравнимое время.

Это наблюдение привело авторов ResNet к идее Shortcut(skip) connection: 
Denote the desired mapping of several layers as H(x). But instead let them fit another mapping F(x) := H(x) - x. 
Более конкретно, это выглядит следующим образом.
Основной building block сети called residual block consists of two convolutional layers with nonlinearity between them and 
shortcutconnection that add the input of the first layer with the output of the second one.
Поэтому если в весах некоторого уровня везде будет 0, он просто пропустит дальше чистый сигнал.

Строя сеть из таких блоков, авторы добиваются того, что 34-слойная сеть показывает лучшее качество, чем 18-слойная как на трейне, так
и на тесте. 

Происходит это потому что skipconnection в какой-то степени позволяет преодолеть проблему vanishing gradients. // Backprop formulas.

Авторы продолжают эксперименты, увеличивая глубину сети. При этом они облегчают сами блоки, заменяя 2 3*3 конволюции одной конволюцией
3*3 и двумя конволюциями 1*1. // Bottleneck picture.

И оказывается, что trained on ImageNet 101 and 152 layer Resnet show more accurate results than 32 layer ResNet.
Однако 1202 слойная сеть обученная на CIFAR показывает большую ошибку на тесте, чем менее глубокая. (при очень маленькой ошибке
на трейне). То есть сеть переобучилась, имея слишком много параметров для небольшого датасета ка CIFAR.

Резнет оказался архитектурой, позволяющей строить очень глубокие сети, которые способны обучаться.

Однако позже выяснилось, что определяющей причиной успеха является не глубина сети, а подходящее число парамтеров.
Архитектура WideResNet, имеющая большее число каналов в блоках конволюции, но гораздо меньшую глубину (8 vs 100)
достигает аналогичного результата за меньшее время за счет эффективной реализации параллельного умножения тензоров.









# Таким образом, to the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to 
# fit an identity mappimg by a stack of nonlinear layers.

# it would be easier for a stack of nonlinear layers to fit zero than an identity mappimg by .


