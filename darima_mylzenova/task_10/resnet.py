ResNet (classification):
	-skipconnection
	-bottleneck	
	-WideResnet
	-SEblock
	-stochastic depth


Сегодня я расскажу об архитектуре Резнет.
Сначала немного поговорим о предпосылках ее появления, затем я опишу архитектуру и в конце поговорим о ее недостатках
и способах их исправления.

После появления таких сетей как VGG, которые показывали улучшение качества с 13, а потом и с 19 слоями, 
исследователи задались вопросом: 

"Is learning better networks as easy as stacking more layers?"
Правда ли , что можно улучшить качество сети, просто добавив в нее еще несколько слоев? 

Однако эксперименты показали, что это не так. Увеличение слоев в VGG демонстрировало худший результат как на тесте, 
так и на трейне. Если бы результат был хуже только на тесте, можно было бы говорить о переобучении.
Но как объяснить большую ошибку на обучении?
Ведь интуиция подсказывает, что более глубокие сети должны обучаться не хуже менее глубоких, т.к.
второе множество вложено в первое. То есть имея сеть глубины N, можно сделать из нее сеть глубины N+k такого же качества,
добавив k Identity layers, пропускающих сигнал без изменений. 
Но на практике обучить более глубокие сети аналогичного качества не получается, по крайней мере, за сравнимое время.

Это наблюдение привело авторов ResNet к идее Shortcut(skip) connection: 

<!-- Denote the desired mapping of several layers as H(x). But instead let them fit another mapping F(x) := H(x) - x. 
Более конкретно, это выглядит следующим образом.
 -->

Обозначим искомое преобразование входного сигнала за H(x). Но вместо этого будем настраивать другое преобразование  F(x) := H(x) - x. Более конкретно, это выглядит следующим образом:

<!-- Основной building block сети called residual block consists of two convolutional layers with nonlinearity between them and 
shortcutconnection that add the input of the first layer with the output of the second one.
 -->
Основной building block сети, называемый residual block, состоит из 2 сверточных слоев с нелинейностью между ними и  
shortcut connection которое берет тензор, пришедший на вход блока и складывает его с выходом последнего слоя блока.

Поэтому, например, если в весах некоторого уровня везде будет 0, он просто пропустит дальше неизмененный сигнал.

Строя сеть из таких блоков, авторы добиваются того, что 34-слойная сеть показывает лучшее качество, чем 18-слойная как на трейне, так и на тесте. 

Происходит это потому что skipconnection в какой-то степени позволяет преодолеть проблему vanishing gradients. // Backprop formulas.

Авторы продолжают эксперименты, увеличивая глубину сети. При этом они облегчают сами блоки, заменяя 2 3х3 конволюции одной конволюцией 3х3 и двумя конволюциями 1х1. // Bottleneck picture.
И оказывается, что trained on ImageNet 101 and 152 layer Resnet show more accurate results than 32 layer ResNet.
Однако 1202 слойная сеть обученная на CIFAR показывает большую ошибку на тесте, чем менее глубокая. (при очень маленькой ошибке на трейне). То есть сеть переобучилась, имея слишком много параметров для небольшого датасета как CIFAR.

Резнет оказался архитектурой, позволяющей строить очень глубокие сети, которые способны эффективно обучаться. Исследователи эксперементировали с глубиной Резнета и им удалось улучшить качество классификации ImageNet еще на несколько десятых долей процента c 1000-слойным резнетом.

Однако позже выяснилось, что определяющей причиной успеха является не глубина сети, а подходящее число парамтеров.
Архитектура WideResNet, имеющая большее число каналов в блоках конволюции, но гораздо меньшую глубину (8 vs 100)
достигает аналогичного результата за меньшее время за счет эффективной реализации параллельного умножения тензоров.















# Таким образом, to the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to 
# fit an identity mappimg by a stack of nonlinear layers.

# it would be easier for a stack of nonlinear layers to fit zero than an identity mappimg by .


ResNet (classification):
	-skipconnection
	-bottleneck	
	-WideResnet
	-SEblock
	-stochastic depth


Сегодня я расскажу об архитектуре Резнет.
Сначала немного поговорим о предпосылках ее появления, затем я опишу архитектуру и в конце поговорим о ее недостатках
и способах их исправления.

 
После появления таких сетей как VGG, которые показаывали улучшенный перфоманс с 19 слоями, 
исследователи задались вопросом: 
"Is learning better networks is as esay as stacking more layers?"

Однако эксперименты показали, что это не так. Увеличение слоев в VGG демонстрировало худший результат как на тесте, 
так и на трейне. Если бы результат был хуже только на тесте, можно было бы говорить о переобучении.
Но как объяснить большую ошибку на обучении?
Ведь интуиция подсказывает, что более глубокие сети должны обучаться не хуже менее глубоких, т.к.
второе множество вложено в первое. Имея сеть глубины N, можно сделать из нее сеть глубины N+k такого же качества,
добавив k Identity layers, пропускающих сигнал без изменений. 
Но на практике обучить более глубокие сети аналогичного качества не получается, по крайней мере, за сравнимое время.

Это наблюдение привело авторов ResNet к идее Shortcut(skip) connection: 
Denote the desired mapping of several layers as H(x). But instead let them fit another mapping F(x) := H(x) - x. 
Более конкретно, это выглядит следующим образом.
Основной building block сети called residual block consists of two convolutional layers with nonlinearity between them and 
shortcutconnection that add the input of the first layer with the output of the second one.
Поэтому если в весах некоторого уровня везде будет 0, он просто пропустит дальше чистый сигнал.

Строя сеть из таких блоков, авторы добиваются того, что 34-слойная сеть показывает лучшее качество, чем 18-слойная как на трейне, так
и на тесте. 

Происходит это потому что skipconnection в какой-то степени позволяет преодолеть проблему vanishing gradients. // Backprop formulas.

Авторы продолжают эксперименты, увеличивая глубину сети. При этом они облегчают сами блоки, заменяя 2 3*3 конволюции одной конволюцией
3*3 и двумя конволюциями 1*1. // Bottleneck picture.

И оказывается, что trained on ImageNet 101 and 152 layer Resnet show more accurate results than 32 layer ResNet.
Однако 1202 слойная сеть обученная на CIFAR показывает большую ошибку на тесте, чем менее глубокая. (при очень маленькой ошибке
на трейне). То есть сеть переобучилась, имея слишком много параметров для небольшого датасета ка CIFAR.

Резнет оказался архитектурой, позволяющей строить очень глубокие сети, которые способны обучаться.

Однако позже выяснилось, что определяющей причиной успеха является не глубина сети, а подходящее число парамтеров.
Архитектура WideResNet, имеющая сравнимое число параметров при большем числе каналов в блоках конволюции и меньшей глубин (8 vs 100)
достигает аналогичного качества за меньшее время за счет эффективной реализации параллельного умножения тензоров.









# Таким образом, to the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to 
# fit an identity mappimg by a stack of nonlinear layers.

# it would be easier for a stack of nonlinear layers to fit zero than an identity mappimg by .





ResNet (classification):
	-skipconnection
	-bottleneck	
	-WideResnet
	-SEblock
	-stochastic depth


Сегодня я расскажу об архитектуре Резнет.
Сначала немного поговорим о предпосылках ее появления, затем я опишу архитектуру и в конце поговорим о ее недостатках
и способах их исправления.

 
После появления таких сетей как VGG, которые показаывали улучшенный перфоманс с 19 слоями, 
исследователи задались вопросом: 
"Is learning better networks is as esay as stacking more layers?"

Однако эксперименты показали, что это не так. Увеличение слоев в VGG демонстрировало худший результат как на тесте, 
так и на трейне. Если бы результат был хуже только на тесте, можно было бы говорить о переобучении.
Но как объяснить большую ошибку на обучении?
Ведь интуиция подсказывает, что более глубокие сети должны обучаться не хуже менее глубоких, т.к.
второе множество вложено в первое. Имея сеть глубины N, можно сделать из нее сеть глубины N+k такого же качества,
добавив k Identity layers, пропускающих сигнал без изменений. 
Но на практике обучить более глубокие сети аналогичного качества не получается, по крайней мере, за сравнимое время.

Это наблюдение привело авторов ResNet к идее Shortcut(skip) connection: 
Denote the desired mapping of several layers as H(x). But instead let them fit another mapping F(x) := H(x) - x. 
Более конкретно, это выглядит следующим образом.
Основной building block сети called residual block consists of two convolutional layers with nonlinearity between them and 
shortcutconnection that add the input of the first layer with the output of the second one.
Поэтому если в весах некоторого уровня везде будет 0, он просто пропустит дальше чистый сигнал.

Строя сеть из таких блоков, авторы добиваются того, что 34-слойная сеть показывает лучшее качество, чем 18-слойная как на трейне, так
и на тесте. 

Происходит это потому что skipconnection в какой-то степени позволяет преодолеть проблему vanishing gradients. // Backprop formulas.

Авторы продолжают эксперименты, увеличивая глубину сети. При этом они облегчают сами блоки, заменяя 2 3*3 конволюции одной конволюцией
3*3 и двумя конволюциями 1*1. // Bottleneck picture.

И оказывается, что trained on ImageNet 101 and 152 layer Resnet show more accurate results than 32 layer ResNet.
Однако 1202 слойная сеть обученная на CIFAR показывает большую ошибку на тесте, чем менее глубокая. (при очень маленькой ошибке
на трейне). То есть сеть переобучилась, имея слишком много параметров для небольшого датасета ка CIFAR.

Резнет оказался архитектурой, позволяющей строить очень глубокие сети, которые способны обучаться.

Однако позже выяснилось, что определяющей причиной успеха является не глубина сети, а подходящее число парамтеров.
Архитектура WideResNet, имеющая сравнимое число параметров при большем числе каналов в блоках конволюции и меньшей глубин (8 vs 100)
достигает аналогичного качества за меньшее время за счет эффективной реализации параллельного умножения тензоров.









# Таким образом, to the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to 
# fit an identity mappimg by a stack of nonlinear layers.

# it would be easier for a stack of nonlinear layers to fit zero than an identity mappimg by .


